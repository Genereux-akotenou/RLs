{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "******* EPISODE 1 *******\n",
      "Episode Reward: 1.0\n",
      "******* EPISODE 2 *******\n",
      "Episode Reward: 1.0\n",
      "******* EPISODE 3 *******\n",
      "Episode Reward: 1.0\n",
      "******* EPISODE 4 *******\n",
      "Episode Reward: 1.0\n",
      "******* EPISODE 5 *******\n",
      "Episode Reward: 1.0\n",
      "Test mean % score = 100\n",
      "Figure(1000x600)\n",
      "Testing completed.\n"
     ]
    }
   ],
   "source": [
    "# FronzenLake\n",
    "\n",
    "!python main.py --env \"FrozenLake\" --algo \"DQN\" --mode \"test\" --model_path \"prebuilt/frozenlake-v1/weights_0150.weights.h5\" --test_episodes 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode:    0/200\t step:   17. Eps: 1.0, reward 0.0\n",
      "Episode:    1/200\t step:   12. Eps: 1.0, reward 0.0\n",
      "Episode:    2/200\t step:   17. Eps: 1.0, reward 0.0\n",
      "Episode:    3/200\t step:   15. Eps: 1.0, reward 0.0\n",
      "Episode:    4/200\t step:   13. Eps: 1.0, reward 0.0\n",
      "Episode:    5/200\t step:    9. Eps: 1.0, reward 0.0\n",
      "Episode:    6/200\t step:    6. Eps: 1.0, reward 0.0\n",
      "Episode:    7/200\t step:    4. Eps: 1.0, reward 0.0\n",
      "Episode:    8/200\t step:    2. Eps: 1.0, reward 0.0\n",
      "Episode:    9/200\t step:    5. Eps: 1.0, reward 0.0\n",
      "Episode:   10/200\t step:   11. Eps: 1.0, reward 0.0\n",
      "Episode:   11/200\t step:   29. Eps: 1.0, reward 0.0\n",
      "Episode:   12/200\t step:    8. Eps: 1.0, reward 0.0\n",
      "Episode:   13/200\t step:    4. Eps: 1.0, reward 0.0\n",
      "Episode:   14/200\t step:    6. Eps: 1.0, reward 0.0\n",
      "Episode:   15/200\t step:    6. Eps: 1.0, reward 0.0\n",
      "Episode:   16/200\t step:    4. Eps: 1.0, reward 0.0\n",
      "Episode:   17/200\t step:    5. Eps: 1.0, reward 0.0\n",
      "Episode:   18/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   19/200\t step:    7. Eps: 0.99, reward 0.0\n",
      "Episode:   20/200\t step:    4. Eps: 0.99, reward 0.0\n",
      "Episode:   21/200\t step:    8. Eps: 0.99, reward 0.0\n",
      "Episode:   22/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   23/200\t step:    5. Eps: 0.99, reward 0.0\n",
      "Episode:   24/200\t step:    5. Eps: 0.99, reward 0.0\n",
      "Episode:   25/200\t step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   26/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   27/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   28/200\t step:   15. Eps: 0.99, reward 0.0\n",
      "Episode:   29/200\t step:    4. Eps: 0.99, reward 0.0\n",
      "Episode:   30/200\t step:    2. Eps: 0.99, reward 0.0\n",
      "Episode:   31/200\t step:    2. Eps: 0.99, reward 0.0\n",
      "Episode:   32/200\t step:    6. Eps: 0.99, reward 0.0\n",
      "Episode:   33/200\t step:    8. Eps: 0.99, reward 0.0\n",
      "Episode:   34/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   35/200\t step:    6. Eps: 0.99, reward 0.0\n",
      "Episode:   36/200\t step:   59. Eps: 0.99, reward 0.0\n",
      "Episode:   37/200\t step:   12. Eps: 0.99, reward 0.0\n",
      "Episode:   38/200\t step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   39/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   40/200\t step:    4. Eps: 0.99, reward 0.0\n",
      "Episode:   41/200\t step:    3. Eps: 0.99, reward 0.0\n",
      "Episode:   42/200\t step:    7. Eps: 0.99, reward 0.0\n",
      "Episode:   43/200\t step:    5. Eps: 0.99, reward 0.0\n",
      "Episode:   44/200\t step:    2. Eps: 0.99, reward 0.0\n",
      "Episode:   45/200\t step:   14. Eps: 0.99, reward 0.0\n",
      "Episode:   46/200\t step:   10. Eps: 0.99, reward 0.0\n",
      "Episode:   47/200\t step:   12. Eps: 0.99, reward 0.0\n",
      "Episode:   48/200\t step:    4. Eps: 0.99, reward 0.0\n",
      "Episode:   49/200\t step:    6. Eps: 0.99, reward 0.0\n",
      "Episode:   50/200\t step:   14. Eps: 0.99, reward 0.0\n",
      "Episode:   51/200\t step:   44. Eps: 0.99, reward 0.0\n",
      "Episode:   52/200\t step:    7. Eps: 0.98, reward 0.0\n",
      "Episode:   53/200\t step:   13. Eps: 0.98, reward 0.0\n",
      "Episode:   54/200\t step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   55/200\t step:    5. Eps: 0.98, reward 0.0\n",
      "Episode:   56/200\t step:   13. Eps: 0.98, reward 0.0\n",
      "Episode:   57/200\t step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   58/200\t step:    6. Eps: 0.98, reward 0.0\n",
      "Episode:   59/200\t step:   15. Eps: 0.98, reward 0.0\n",
      "Episode:   60/200\t step:    5. Eps: 0.98, reward 0.0\n",
      "Episode:   61/200\t step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   62/200\t step:    9. Eps: 0.98, reward 0.0\n",
      "Episode:   63/200\t step:   24. Eps: 0.98, reward 0.0\n",
      "Episode:   64/200\t step:   13. Eps: 0.98, reward 0.0\n",
      "Episode:   65/200\t step:    2. Eps: 0.98, reward 0.0\n",
      "Episode:   66/200\t step:    8. Eps: 0.98, reward 0.0\n",
      "Episode:   67/200\t step:    6. Eps: 0.98, reward 0.0\n",
      "Episode:   68/200\t step:    7. Eps: 0.98, reward 0.0\n",
      "Episode:   69/200\t step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   70/200\t step:   12. Eps: 0.98, reward 0.0\n",
      "Episode:   71/200\t step:   17. Eps: 0.98, reward 0.0\n",
      "Episode:   72/200\t step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   73/200\t step:    6. Eps: 0.98, reward 0.0\n",
      "Episode:   74/200\t step:    4. Eps: 0.98, reward 0.0\n",
      "Episode:   75/200\t step:   10. Eps: 0.98, reward 0.0\n",
      "Episode:   76/200\t step:    9. Eps: 0.98, reward 0.0\n",
      "Episode:   77/200\t step:   40. Eps: 0.98, reward 0.0\n",
      "Episode:   78/200\t step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   79/200\t step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   80/200\t step:    7. Eps: 0.98, reward 0.0\n",
      "Episode:   81/200\t step:    4. Eps: 0.98, reward 0.0\n",
      "Episode:   82/200\t step:   16. Eps: 0.98, reward 0.0\n",
      "Episode:   83/200\t step:   13. Eps: 0.98, reward 0.0\n",
      "Episode:   84/200\t step:    5. Eps: 0.98, reward 0.0\n",
      "Episode:   85/200\t step:    8. Eps: 0.98, reward 0.0\n",
      "Episode:   86/200\t step:   12. Eps: 0.98, reward 0.0\n",
      "Episode:   87/200\t step:    2. Eps: 0.97, reward 0.0\n",
      "Episode:   88/200\t step:    1. Eps: 0.97, reward 0.0\n",
      "Episode:   89/200\t step:    6. Eps: 0.97, reward 0.0\n",
      "Episode:   90/200\t step:    6. Eps: 0.97, reward 0.0\n",
      "Episode:   91/200\t step:    8. Eps: 0.97, reward 0.0\n",
      "Episode:   92/200\t step:   11. Eps: 0.97, reward 0.0\n",
      "Episode:   93/200\t step:    4. Eps: 0.97, reward 0.0\n",
      "Episode:   94/200\t step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   95/200\t step:    6. Eps: 0.97, reward 0.0\n",
      "Episode:   96/200\t step:   17. Eps: 0.97, reward 0.0\n",
      "Episode:   97/200\t step:   31. Eps: 0.97, reward 0.0\n",
      "Episode:   98/200\t step:   11. Eps: 0.97, reward 0.0\n",
      "Episode:   99/200\t step:    1. Eps: 0.97, reward 0.0\n",
      "Episode:  100/200\t step:    2. Eps: 0.97, reward 0.0\n",
      "Episode:  101/200\t step:    4. Eps: 0.97, reward 0.0\n",
      "Episode:  102/200\t step:    4. Eps: 0.97, reward 0.0\n",
      "Episode:  103/200\t step:    5. Eps: 0.97, reward 0.0\n",
      "Episode:  104/200\t step:    9. Eps: 0.97, reward 0.0\n",
      "Episode:  105/200\t step:   12. Eps: 0.97, reward 0.0\n",
      "Episode:  106/200\t step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:  107/200\t step:    9. Eps: 0.97, reward 0.0\n",
      "Episode:  108/200\t step:   10. Eps: 0.97, reward 0.0\n",
      "Episode:  109/200\t step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:  110/200\t step:   17. Eps: 0.97, reward 0.0\n",
      "Episode:  111/200\t step:   10. Eps: 0.97, reward 0.0\n",
      "Episode:  112/200\t step:    1. Eps: 0.97, reward 0.0\n",
      "Episode:  113/200\t step:    4. Eps: 0.97, reward 0.0\n",
      "Episode:  114/200\t step:    3. Eps: 0.97, reward 0.0\n"
     ]
    }
   ],
   "source": [
    "!python main.py --env \"FrozenLake\" --algo \"DQN\" --mode \"train\" --output_dir \"prebuilt/frozenlake-v1-8x8/\" --batch_size 32 --n_episodes 200 --max_steps 300 --map \"FFFFFFFF\" \"FHFHFFFF\" \"FFFHFFFF\" \"FHFFFHFF\" \"FFFFHFFF\" \"FHFHFFFF\" \"FHFHFHFF\" \"FFFFFHFG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
